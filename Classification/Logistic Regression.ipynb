{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistics Regression is a supervised machine learning algorithm which is used when the target variable is categorical. Don't let the 'Regression' in the name fool you. This algorithm is normally used for classification.\n",
    "\n",
    "Logistic Regression can be considered as a variation of linear regression where we use a sigmoid function as the hypothetical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "h( x ) = sigmoid( mx + c )\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Here, m is the weight vector.<br>\n",
    "x is the feature vector. <br>\n",
    "c is the bias.\n",
    "\n",
    "<center>$sigmoid(z) = \\frac{1}{1 + e^(-z)}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Intuition\n",
    "\n",
    "The cost function that we use for the logistic regression so that the target value is between 0 and 1 is : <br>\n",
    "<br>\n",
    "<center>$ J = -y(log(h(x)) - (1 - y)log(1 - h(x)) $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent is the same as that of a linear regression with steps and weights being reduced by using the learning rate until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We use the same datasets of iris that we have used in <a href=\"https://github.com/RitwickSV/ML-Models-From-Scratch/blob/main/Classification/K%20Nearest%20Neighbors.ipynb\"> KNN </a>  classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings( \"ignore\" )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg: #Model For Logistic Regression from scratch\n",
    "    def __init__(self, learning_rate, num_iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "    \n",
    "    #Model training\n",
    "    def fit(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.examples, self.features = np.shape(X)\n",
    "        \n",
    "        #initialising weight to 0\n",
    "        self.M = np.zeros(self.features)\n",
    "        self.c = 0\n",
    "        \n",
    "        #gradient descent\n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            H = 1 / (1 + np.exp( - (self.X.dot(self.M) + self.c)))\n",
    "            \n",
    "            #calculate the gradients\n",
    "            tmp = ( H - self.Y.T )        \n",
    "            tmp = np.reshape( tmp, self.examples )        \n",
    "            df_M = np.dot( self.X.T, tmp ) / self.examples         \n",
    "            df_c = np.sum( tmp ) / self.examples \n",
    "        \n",
    "            #update weights and intercept until convergence\n",
    "            self.M = self.M - self.learning_rate * df_M    \n",
    "            self.c = self.c - self.learning_rate * df_c\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    #Model Prediction\n",
    "    def predict(self, X):\n",
    "        H = 1 / ( 1 + np.exp( - ( X.dot( self.M ) + self.c ) ) )        \n",
    "        Y = np.where( H > 0.5, 1, 0 )        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished generating our model for logistic regression using the learning rate and the number of iterations.\n",
    "Let's call and check the model and compare it to the model present in `sklearn.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "#Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training both the models\n",
    "model = LogReg( learning_rate = 0.01, num_iterations = 10000 )\n",
    "model.fit( X_train, Y_train )\n",
    "\n",
    "model1 = LogisticRegression()    \n",
    "model1.fit( X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "Y_pred = model.predict( X_test )    \n",
    "Y_pred1 = model1.predict( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the models' performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set by our model     :   43.333333333333336\n",
      "Accuracy on test set by sklearn model   :   100.0\n"
     ]
    }
   ],
   "source": [
    "# measure performance    \n",
    "correctly_classified = 0    \n",
    "correctly_classified1 = 0\n",
    "\n",
    "# counter    \n",
    "count = 0    \n",
    "for count in range( np.size( Y_pred ) ) :  \n",
    "\n",
    "    if Y_test[count] == Y_pred[count] :            \n",
    "        correctly_classified = correctly_classified + 1\n",
    "\n",
    "    if Y_test[count] == Y_pred1[count] :            \n",
    "        correctly_classified1 = correctly_classified1 + 1\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "print( \"Accuracy on test set by our model     :  \", ( \n",
    "  correctly_classified / count ) * 100 )\n",
    "print( \"Accuracy on test set by sklearn model   :  \", ( \n",
    "  correctly_classified1 / count ) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
